# @package _global_

defaults:
  - plugins:
    - deepspeed.yaml
  - logger: tensorboard.yaml
  - trainer: default.yaml
  - _self_
  - experiment: null

# Settings
output_directory: "Chemformer-main-MolecularAI/Chemformer-main-MolecularAI/experiment_1/fine_tune_logs"
deepspeed_config_path: "ds_config.json"

# Trainer
seed: 73
resume: false
batch_size: 64
n_epochs: 100
limit_val_batches: 1.0
n_buckets: 24
n_gpus: 1
n_nodes: 1
acc_batches: 16
accelerator: null
check_val_every_n_epoch: 1

# Data
data_path: /mnt/c/Users/lenovo/Chemformer-main-MolecularAI/Chemformer-selected/data/seq-to-seq_datasets/uspto_50_1000.pickle  # for linux use
#data_path: C:/Users/lenovo/Chemformer-main-MolecularAI/Chemformer-selected/data/seq-to-seq_datasets/uspto_50_1000.pickle     # for windows use
vocabulary_path: bart_vocab_downstream.json
task: backward_prediction
augmentation_probability: 0.0
augmentation_strategy: all
data_device: cuda

# Model
model_path: /mnt/c/Users/lenovo/Chemformer-main-MolecularAI/Chemformer-selected/models/fined-tuned/uspto_50/last.ckpt
#model_path: C:/Users/lenovo/Chemformer-main-MolecularAI/Chemformer-selected/models/fined-tuned/uspto_50/last.ckpt
model_type: bart
learning_rate: 0.0003
weight_decay: 0.0
clip_grad: 1.0
d_model: 512
n_layers: 6
n_heads: 8
d_feedforward: 2048
train_tokens: null
train_mode: training
n_beams: 1

schedule: cycle
warm_up_steps: 8000

datamodule:
  - Uspto50DataModule

callbacks:
  ModelCheckpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: validation_loss
    mode: min
    save_top_k: 1
    filename: "best-{epoch}-{validation_loss:.2f}"
    
  ValidationScoreCallback:
    _target_: molbart.utils.callbacks.ValidationScoreCallback
    
  StepCheckpoint:
    _target_: molbart.utils.callbacks.StepCheckpoint
    
  OptLRMonitor:
    _target_: molbart.utils.callbacks.OptLRMonitor

scorers:
  - FractionInvalidScore
  - TopKAccuracyScore